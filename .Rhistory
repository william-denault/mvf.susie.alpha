verbose = TRUE
maxit = 100
tol = 1e-3
cov_lev = 0.95
min.purity = 0.5
L_start = 3
filter.cs = TRUE
init_pi0_w = 1
nullweight = 10
control_mixsqp = list(
eps = 1e-6,
numiter.em = 40,
verbose = FALSE
)
cal_obj = FALSE
greedy = TRUE
backfit = TRUE
max_SNP_EM = 1000
gridmult = sqrt(2)
max_scale = 10
max_step_EM = 1
cor_small = FALSE
filter.number = 10
family = "DaubLeAsymm"
e = 0.001
pt <- proc.time()
if(L_start >L)
{
L_start <- L
}
if(verbose){
print("Starting initialization")
}
if(post_processing=="TI"){
TI  <- TRUE
HMM <- FALSE
}
if(post_processing=="HMM"){
TI  <- FALSE
HMM <- TRUE
}
if(post_processing=="none"){
TI  <- FALSE
HMM <- FALSE
}
ind_analysis <- which_notNA_pos(Y)
#remove column of X constant in some sub cases
tidx <- check_cst_X_sub_case(X,ind_analysis)
if( length(tidx)>0){
warning(paste("Some of the columns of X are constants, we removed" ,length(tidx), "columns"))
X <- X[,-tidx]
}
m1 <- multfsusie(Y=Y,
X=X,
pos=pos,
L=11 ,
nullweight=10,
maxit=10 ,
post_processing = "HMM")
rm(list=ls()
)
devtools::load_all(".")
library(mvf.susie.alpha)
set.seed(1)
N <- 100  #Sample size
P= 10 # number of SNP
L <-2
list_lev_res <- list(5,6) # two functional phenotypes ,
#one of length 2^5, and one of length 2^6)
n_univ <- 3 #3 univariate phenotypes
eff <-  list()
for(l in 1:L){ #Simulate the mult-trait effect
eff[[l]] <-   simu_effect_multfsusie (list_lev_res=list_lev_res,
n_univ=n_univ, output_level = 2)
}
eff[[1]]$func_effect[[1]]$sim_func <- eff[[l]]$func_effect[[1]]$sim_func*0
eff[[1]]$func_effect[[1]]$sim_func [ 10] <- 2
eff[[2]]$func_effect[[1]]$sim_func <- eff[[l]]$func_effect[[1]]$sim_func*0
eff[[2]]$func_effect[[1]]$sim_func [ 20] <- 2
Y_f1 <-  matrix(rnorm((2^list_lev_res[[1]])*N ,sd=5), nrow = N)
Y_f2 <-  matrix(rnorm((2^list_lev_res[[2]])*N ,sd=5), nrow = N)
Y_u <- matrix(rnorm((n_univ)*N ,sd=1), nrow = N)
G = matrix(sample(c(0, 1,2), size=N*P, replace=TRUE), nrow=N, ncol=P) #Genotype
true_pos <- sample( 1:ncol(G), L)# actually causal column/SNP
for ( i in 1:N){
for ( l in 1:L){
Y_f1[i,]<- Y_f1[i,]+eff[[l]]$func_effect[[1]]$sim_func*G[i,true_pos[[l]]]
Y_f2[i,]<- Y_f2[i,]+eff[[l]]$func_effect[[2]]$sim_func*G[i,true_pos[[l]]]
Y_u[i,]<- Y_u[i,]+ eff[[l]]$univ_effect*G[i,true_pos[[l]]]
}
}
Y_f <- list()
Y_f1[1:20,1] <-NA ### heree pb because NA in the first modality
X <- G
X[,1] <- 1
X[1:20,1] <- 2### and X constant in the first modality
Y_f[[1]] <- Y_f1
Y_f[[2]] <- Y_f2
Y <- list( Y_f = Y_f, Y_u=Y_u) # preparing data ,
#current ouput type expect list of two which element named
#Y_f for functional trait and Y_u for univariate trait
ind_analysis <- which_notNA_pos(Y)
#remove column of X constant in some sub cases
tidx <- check_cst_X_sub_case(X,ind_analysis)
print(tidx)
pos = list(pos1= 1: ncol(Y$Y_f[[1]]),
pos2= 1: ncol(Y$Y_f[[2]])) # if you signal is sample between 1 and 64
m1 <- multfsusie(Y=Y,
X=X,
pos=pos,
L=11 ,
nullweight=10,
maxit=10 ,
post_processing = "HMM")
L = 2
pos = NULL
prior = "mixture_normal"
post_processing = "TI"
verbose = TRUE
maxit = 100
tol = 1e-3
cov_lev = 0.95
min.purity = 0.5
L_start = 3
filter.cs = TRUE
init_pi0_w = 1
nullweight = 10
control_mixsqp = list(
eps = 1e-6,
numiter.em = 40,
verbose = FALSE
)
cal_obj = FALSE
greedy = TRUE
backfit = TRUE
max_SNP_EM = 1000
gridmult = sqrt(2)
max_scale = 10
max_step_EM = 1
cor_small = FALSE
filter.number = 10
family = "DaubLeAsymm"
e = 0.001
pt <- proc.time()
if(L_start >L)
{
L_start <- L
}
if(verbose){
print("Starting initialization")
}
if(post_processing=="TI"){
TI  <- TRUE
HMM <- FALSE
}
if(post_processing=="HMM"){
TI  <- FALSE
HMM <- TRUE
}
if(post_processing=="none"){
TI  <- FALSE
HMM <- FALSE
}
ind_analysis <- which_notNA_pos(Y)
#remove column of X constant in some sub cases
tidx <- check_cst_X_sub_case(X,ind_analysis)
if( length(tidx)>0){
warning(paste("Some of the columns of X are constants, we removed" ,length(tidx), "columns"))
X <- X[,-tidx]
}
if(verbose){
print("Data transform")
}
h <- 1
list_wdfs <- list()
list_indx_lst  <-  list()
if( !is.null(Y$Y_f)){
outing_grid <- list()
if( is.null(pos)){
pos <- list()
for (i in 1:length(Y$Y_f))
{
pos[[i]] <- 1:ncol(Y$Y_f[[i]])
}
}
for (i in 1:length(Y$Y_f)){
if ( !(length(pos[[i]])==ncol(Y$Y_f[[i]]))) #miss matching positions and number of observations
{
stop(paste("Error: number of position provided different from the number of column of Y$Y_f, entry",i))
}
}
interpolated_Y <- Y
for ( k in 1:length(Y$Y_f))
{
map_data <-  fsusieR::remap_data(Y=Y$Y_f[[k]],
pos=pos[[k]],
verbose=verbose)
outing_grid[[k]] <- map_data$outing_grid
interpolated_Y$Y_f[[k]] <-  map_data$Y
temp               <- DWT2( map_data$Y,
filter.number = filter.number,
family        = family)
list_wdfs[[h]]     <- cbind( temp$D,temp$C)
list_indx_lst[[h]] <- fsusieR:::gen_wavelet_indx( log2(ncol(  list_wdfs[[h]]) ))
h <- h+1
rm(map_data)
}
Y_f <- list_wdfs
v1  <- nrow( Y_f [[1]])
}else{
Y_f <- NULL
v1  <- nrow( Y$Y_u)
outing_grid <- NULL
}
Y_data   <- list(Y_u =Y$Y_u,
Y_f =Y_f)
type_mark <- is.functional ( Y=Y_data  )
#### centering and scaling covariate ----
X <- fsusieR:::colScale(X)
# centering input
Y_data <- multi_array_colScale(Y_data, scale=FALSE)
#
if(verbose){
print("Data transform done")
}
#### discarding  null/low variance    ------
if( missing(thresh_lowcount)){
threshs <- create_null_thresh(type_mark = type_mark)
}else{
threshs <- thresh_lowcount
}
threshs <- create_null_thresh(type_mark = type_mark)
low_trait <- check_low_count  (Y_data,
thresh_lowcount = threshs,
ind_analysis    = ind_analysis
)
v1 <- rep( 1, nrow(X))
if(verbose){
print("Initializing prior")
}
if( cor_small){
df <- list()
if( !is.null(ind_analysis$idx_u)){
df$Y_u <- lengths(ind_analysis$idx_u)-1
}else{
df$Y_u <- NULL
}
if( !is.null(ind_analysis$idx_f)){
df$Y_f <- lengths(ind_analysis$idx_f)-1
}else{
df$Y_f <- NULL
}
}else{
df= NULL
}
temp  <- init_prior_multfsusie(Y              = Y_data ,
X              = X,
v1             = v1,
prior          = prior,
list_indx_lst  = list_indx_lst,
low_trait      = low_trait,
control_mixsqp = control_mixsqp,
nullweight     = nullweight,
ind_analysis   = ind_analysis,
max_SNP_EM     = max_SNP_EM,
gridmult       = gridmult,
max_step_EM    = max_step_EM
)
Y_data
X
ind_analysis
if(is.null(Y$Y_u)){
G_prior_u <- NULL
res_u   <- NULL
}else{
if(missing(ind_analysis )){
res_u   <- fsusieR:::cal_Bhat_Shat(Y$Y_u,X,v1)
}else{
res_u   <- fsusieR:::cal_Bhat_Shat(Y$Y_u,X,v1,lowc_wc=NULL,ind_analysis =ind_analysis$idx_u)
}
if (is.null(low_trait$low_u)){
G_prior_u <- lapply(1:ncol(Y$Y_u), function(j) ashr::ash(res_u$Bhat[,j],
res_u$Shat[,j] ,
mixcompdist = "normal" ,
outputlevel=0,
gridmult=gridmult)
)
}else{
G_prior_u <- lapply((1:ncol(Y$Y_u))[- low_trait$low_u], function(j) ashr::ash(res_u$Bhat[,j],
res_u$Shat[,j],
mixcompdist = "normal" ,
outputlevel=0,
gridmult=gridmult)
)
}
#### TODO add optimization step here to bypass for effect fitting
for ( k in 1: length(G_prior_u)){
attr(G_prior_u[[k]], "class")  <- "mixture_normal"
}
}
is.null(Y$Y_u)
fsusieR:::cal_Bhat_Shat(Y$Y_u,X,v1,lowc_wc=NULL,ind_analysis =ind_analysis$idx_u)
ind_analysis$idx_u
fsusieR:::cal_Bhat_Shat
v1
fsusieR:::cal_Bhat_Shat(Y$Y_u,X,v1=v1,lowc_wc=NULL,ind_analysis =ind_analysis$idx_u)
fsusieR:::cal_Bhat_Shat(Y$Y_u,X,v1=v1)
ind_analysis$idx_u
fsusieR:::cal_Bhat_Shat
is.list(ind_analysis)
Bhat <-  do.call(cbind,lapply(1:length(ind_analysis),
function(l){
d   <- colSums(X[ind_analysis[[l]], ]^2)
out <- (t(X[ind_analysis[[l]], ])%*%Y[ind_analysis[[l]], l])/d
return(out)
}
) )
Bhat <-  do.call(cbind,lapply(1:length(ind_analysis),
function(l){
d   <- colSums(X[ind_analysis[[l]], ]^2)
out <- (t(X[ind_analysis[[l]], ])%*%Y[ind_analysis[[l]], l])/d
return(out)
}
) )
ind_analysis0= ind_analysis$idx_u
Bhat <-  do.call(cbind,lapply(1:length(ind_analysis),
function(l){
d   <- colSums(X[ind_analysis[[l]], ]^2)
out <- (t(X[ind_analysis[[l]], ])%*%Y[ind_analysis[[l]], l])/d
return(out)
}
) )
ind_analysis0= ind_analysis$idx_u
Bhat <-  do.call(cbind,lapply(1:length(ind_analysis0),
function(l){
d   <- colSums(X[ind_analysis0[[l]], ]^2)
out <- (t(X[ind_analysis[[l]], ])%*%Y[ind_analysis0[[l]], l])/d
return(out)
}
) )
out <- (t(X[ind_analysis0[[l]], ])%*%Y[ind_analysis0[[l]], l])/d
out <- (t(X[ind_analysis0[[l]], ])%*%Y$Y_u[ind_analysis0[[l]], l])/d
res_u   <- fsusieR:::cal_Bhat_Shat(Y$Y_u[,1],X,v1=v1,lowc_wc=NULL,ind_analysis =ind_analysis$idx_u)
ind_analysis0= ind_analysis$idx_u
Bhat <-  do.call(cbind,lapply(1:length(ind_analysis0),
function(l){
d   <- colSums(X[ind_analysis0[[l]], ]^2)
out <- (t(X[ind_analysis0[[l]], ])%*%Y$Y_u[ind_analysis0[[l]], l])/d
return(out)
}
) )
res_u   <- fsusieR:::cal_Bhat_Shat(Y$Y_u ,X,v1=v1,lowc_wc=NULL,ind_analysis =ind_analysis$idx_u)
ind_analysis0= ind_analysis$idx_u
Bhat <-  do.call(cbind,lapply(1:length(ind_analysis0),
function(l){
d   <- colSums(X[ind_analysis0[[l]], ]^2)
out <- (t(X[ind_analysis0[[l]], ])%*%Y$Y_u[ind_analysis0[[l]], l])/d
return(out)
}
) )
Shat  <-   matrix(mapply(function(l,j)
sqrt(Rfast::cova(Y$Y_u[ind_analysis0[[l]],l] - X[ind_analysis0[[l]], j]  *  Bhat[j,l]) /(length(ind_analysis0[[l]])-1)),
l=rep(1:dim(Y$Y_u)[2],each= ncol(X)),
j=rep(1:dim(X)[2], ncol(Y$Y_u))
),
ncol=dim(Y)[2]
)
l=1j=1
l=1;j=1
(length(ind_analysis0[[l]])-1)
X[ind_analysis0[[l]], j]  *  Bhat[j,l]
Y$Y_u[ind_analysis0[[l]],l] - X[ind_analysis0[[l]], j]  *  Bhat[j,l]
Rfast::cova(Y$Y_u[ind_analysis0[[l]],l] - X[ind_analysis0[[l]], j]  *  Bhat[j,l])
cova
?Rfat::cova
?Rfast::cova
str((Y$Y_u[ind_analysis0[[l]],l] - X[ind_analysis0[[l]], j]  *  Bhat[j,l]))
covastr((Y$Y_u[ind_analysis0[[l]],l] - X[ind_analysis0[[l]], j]  *  Bhat[j,l]))
cov((Y$Y_u[ind_analysis0[[l]],l] - X[ind_analysis0[[l]], j]  *  Bhat[j,l]))
cova((Y$Y_u[ind_analysis0[[l]],l] - X[ind_analysis0[[l]], j]  *  Bhat[j,l]))
Rfast::cova((Y$Y_u[ind_analysis0[[l]],l] - X[ind_analysis0[[l]], j]  *  Bhat[j,l]))
var((Y$Y_u[ind_analysis0[[l]],l] - X[ind_analysis0[[l]], j]  *  Bhat[j,l]))
Rfast::cova(t(Y$Y_u[ind_analysis0[[l]],l] - X[ind_analysis0[[l]], j]  *  Bhat[j,l])))
Rfast::cova(t(Y$Y_u[ind_analysis0[[l]],l] - X[ind_analysis0[[l]], j]  *  Bhat[j,l]))
Rfast::cova(matrix(Y$Y_u[ind_analysis0[[l]],l] - X[ind_analysis0[[l]], j]  *  Bhat[j,l]))
# Load necessary libraries
library(microbenchmark)
library(matrixStats)
library(Rfast)
# Create a sample matrix
set.seed(123)
sample_matrix <- matrix(rnorm(1e6), ncol = 1000)
# Define the functions to benchmark
colSds_function <- function() {
matrixStats::colSds(sample_matrix)
}
cova_function <- function() {
Rfast::cova(sample_matrix)
}
# Run the microbenchmark
benchmark_result <- microbenchmark(
colSds = colSds_function(),
cova = cova_function(),
times = 100
)
# Print the benchmark result
print(benchmark_result)
matrixStats::colSds(sample_matrix)
Rfast::cova(sample_matrix)
# Load necessary libraries
library(microbenchmark)
library(matrixStats)
library(Rfast)
# Create a sample matrix
set.seed(123)
sample_matrix <- matrix(rnorm(1e6), ncol = 1000)
# Define the functions to benchmark
colSds_function <- function() {
matrixStats::colSds(sample_matrix)
}
cova_function <- function() {
Rfast::colVars(sample_matrix)
}
# Run the microbenchmark
benchmark_result <- microbenchmark(
colSds = colSds_function(),
cova = cova_function(),
times = 100
)
# Print the benchmark result
print(benchmark_result)
Rfast::cova(matrix(Y$Y_u[ind_analysis0[[l]],l] - X[ind_analysis0[[l]], j]  *  Bhat[j,l]))
# Define the functions to benchmark
colSds_function <- function() {
matrixStats::colSds(sample_matrix)
}
cova_function <- function() {
sqrt(Rfast::colVars(sample_matrix))
}
# Run the microbenchmark
benchmark_result <- microbenchmark(
colSds = colSds_function(),
cova = cova_function(),
times = 100
)
# Print the benchmark result
print(benchmark_result)
sqrt(Rfast::colVars(sample_matrix))
plot( matrixStats::colSds(sample_matrix), sqrt(Rfast::colVars(sample_matrix)))
x <- matrix(as.logical(rbinom(100*100,1,0.5)),100,100)
a<-colAny(x)
#b<-apply(x,2,any)
#all.equal(a,b)
a<-rowAny(x)
#b<-apply(x,1,any)
#all.equal(a,b)
a<-colAll(x)
#b<-apply(x,2,all)
#all.equal(a,b)
a<-b<-x<-NULL
a
colAny(x)
x <- matrix(as.logical(rbinom(100*100,1,0.5)),100,100)
colAny(x)
# Create a sample matrix
set.seed(123)
sample_matrix <- matrix(rnorm(1e6), ncol = 1000)
# Define the functions to benchmark
colSds_function <- function() {
matrixStats::colSums(sample_matrix)
}
cova_function <- function() {
sqrt(Rfast::colsums(sample_matrix))
}
# Run the microbenchmark
benchmark_result <- microbenchmark(
colSds = colSds_function(),
cova = cova_function(),
times = 100
)
# Create a sample matrix
set.seed(123)
sample_matrix <- matrix(rnorm(1e6), ncol = 1000)
# Define the functions to benchmark
colSds_function <- function() {
matrixStats::colSums2(sample_matrix)
}
cova_function <- function() {
sqrt(Rfast::colsums(sample_matrix))
}
# Run the microbenchmark
benchmark_result <- microbenchmark(
colSds = colSds_function(),
cova = cova_function(),
times = 100
)
matrixStats::colSums2(sample_matrix)
# Define the functions to benchmark
colSds_function <- function() {
matrixStats::colSums2(sample_matrix)
}
cova_function <- function() {
(Rfast::colsums(sample_matrix))
}
# Run the microbenchmark
benchmark_result <- microbenchmark(
colSds = colSds_function(),
cova = cova_function(),
times = 100
)
# Print the benchmark result
print(benchmark_result)
